{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn 모델 생성 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0           5.1          3.5           1.4          0.2       0\n",
       "1           4.9          3.0           1.4          0.2       0\n",
       "2           4.7          3.2           1.3          0.2       0\n",
       "3           4.6          3.1           1.5          0.2       0\n",
       "4           5.0          3.6           1.4          0.2       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iris datasets 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "iris_data  = iris.data # feature\n",
    "iris_label = iris.target # label\n",
    "\n",
    "iris_columns = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "iris_pdf=pd.DataFrame(iris_data, columns=iris_columns)\n",
    "iris_pdf['target'] = iris_label\n",
    "iris_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 훈련세트 데이터세트 분할 및 모델 생성\n",
    "from sklearn.tree import DecisionTreeClassifier # Estimator\n",
    "from sklearn.model_selection import train_test_split # RandomSpliter : 데이터를 무작위로 섞고 일정비율에 따라 잘라준다.\n",
    "\n",
    "#데이터 무작위 섞고 비율로 나눠준다.\n",
    "x_train, x_test, t_train, t_test = train_test_split(\n",
    "    iris_data,\n",
    "    iris_label,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 모델 생성\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "# 훈련! log_reg 모델 자체에서 훈련이 일어나게 된다.\n",
    "tree_clf.fit(x_train,t_train) \n",
    "\n",
    "pred = tree_clf.predict(x_test)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``Spark ML 사용해 보기``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.master('local').appName('tree-classifier').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|     0|\n",
      "|         4.9|        3.0|         1.4|        0.2|     0|\n",
      "|         4.7|        3.2|         1.3|        0.2|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "|         5.0|        3.6|         1.4|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas 데이터 프레임을 spark 데이터 프레임으로 -> createDataFrame\n",
    "iris_sdf = spark.createDataFrame(iris_pdf)\n",
    "iris_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark DataFrame 에서 훈련, 테스트 데이터 분할 - randomSplit 메소드 활용\n",
    "train_sdf, test_sdf = iris_sdf.randomSplit([0.8,0.2],seed=42) # train_data 비율 80% (test_data 비율 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 데이터 세트는 어떻게 변환이 되어도 하나만 존재하는 게 좋다.\n",
    "- 훈련 데이터가 모델에 들어가면 모델이 transform을 해서 예측값이 나온다.\n",
    "- 여러번 훈련 하면 transform 여러 번 일어난다.\n",
    "- train_sdf가 메모리 내에 여러 개가 생길 수도 있다. \n",
    "- 전처리 -> 훈련 -> 예측 -> 평가 -> 훈련 -> 예측 -> 평가 -> 훈련 -> 예측 -> 평가\n",
    "=> RDD가 여러개 생긴다. => 캐시에 넣고 활용하는 것이 좋다.\n",
    "\n",
    " ``cache를 적용하지 않으면 훈련을 할 때마다 sdf가 여러개 생긴다. => 좋은 방법 아니다.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, target: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 직전에 사용할 데이터는 캐싱을 하는 것이 유리!\n",
    "train_sdf.cache()\n",
    "# => 머신러닝 데이터로 사용 할 준비가 될 데이터는 cahce에 넣고 활용하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorAssembler를 이용하여 모든 feature 컬럼들을 하나의 feature vector로 변환\n",
    "#### 하나의 백터로 합쳐주는 역할[4.3,3.0,1.1,0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train 세트 transform : ``train_feature_vector_sdf=vec_assembler.transform(train_sdf)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|[4.3,3.0,1.1,0.1]|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|[4.4,2.9,1.4,0.2]|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|[4.4,3.2,1.3,0.2]|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|[4.5,2.3,1.3,0.3]|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|[4.6,3.1,1.5,0.2]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 합쳐질 컬럼을 작성\n",
    "iris_columns = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "\n",
    "# VectorAssembler로 데이터 프레임에 있는 데이터를 하나의 백터로 합쳐주기\n",
    "vec_assembler=VectorAssembler(inputCols=iris_columns,outputCol=\"features\")\n",
    "\n",
    "# vectorAssembler_transform \n",
    "\n",
    "train_feature_vector_sdf=vec_assembler.transform(train_sdf)\n",
    "train_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성 : ``dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"target\", maxDepth=5)``\n",
    "#### 모델 학습 : ``dt_model = dt.fit(train_feature_vector_sdf)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.classification.DecisionTreeClassifier'>\n",
      "<class 'pyspark.ml.classification.DecisionTreeClassificationModel'>\n"
     ]
    }
   ],
   "source": [
    "# Spark ML의 모델은 Estimator지만, 데이터를 변환시키는 Transformer이다.\n",
    "# train 데이터를 받아서 예측 값(pred)으로 변환시키는 transform 과정이 일어난다.\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# 모델 생성. 어떤 컬럼의 데이터를 이용해서 학습을 할지 결정을 지어 준다.\n",
    "# 예측을 ~할거야\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"target\", maxDepth=5) # 나무 깊이 5단계\n",
    "\n",
    "# 모델 학습. fit() 메소드를 이용하여 학습을 수행하고, 그 결과를 ML 모델로 반환한다.\n",
    "dt_model = dt.fit(train_feature_vector_sdf)\n",
    "\n",
    "# scikitlearn은 fit을 하게되면 모델 자체에서 훈련이 수행\n",
    "# spark 모델은 fit을 하게 되면 그 결과로만 모델이 나온다.\n",
    "print(type(dt))\n",
    "print(type(dt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 이용해서 예측\n",
    "test_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train 세트 transform : train_feature_vector_sdf=vec_assembler.transform(train_sdf)\n",
    "#### test 세트 transform : **``test_feature_vector_sdf= vec_assembler.transform(test_sdf)``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터에서 적용시켰던 Transformer를 테스트 세트에 그대로 적용시킨다.\n",
    "test_feature_vector_sdf= vec_assembler.transform(test_sdf)\n",
    "test_feature_vector_sdf.show(5)\n",
    "# 훈련세트의 transform은 테스트세트에서도  그대로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측 : predictions=dt_model.transform(test_feature_vector_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "# ==> 테스트 데이터 + 변환된 feature vector로 구성된 DataFrame을 활용한다.\n",
    "# feature vector 란? vector_assembler 에 의해서 합쳐진 형태의 벡터를 활용한다.\n",
    "# 모델의 transform() 메소드를 활용한다.\n",
    "predictions=dt_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터들이 계속 합쳐지는 느낌이다. ==> RDD transformation이다.\n",
    "# rawPrediction/ probability / prediction 추가된다.\n",
    "# 기존 데이터프레임을 수정시키고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``rawPrediction`` : 머시러닝 모델 알고리즘 별로 다를 수 있다.\n",
    "    - 머신러닝 알고리즘에 의해서 계산된 값\n",
    "    - 값에 대한 정확한 의미는 없다!!!\n",
    "    - LogisticRegression 의 경우 예측 label 별로, 예측 수행 전 ``sigmoid`` 함수 적용 전 값 (sigmoid는 0~1 사이 숫자)\n",
    "        * $\\hat{y} = \\sigma(WX + b)$\n",
    "        * $WX + b$의 결과가 `rawPrediction`\n",
    "* `probability` : 예측 label 별 예측 확률\n",
    "\n",
    "- ex)0번 클래스 100프로 예측\n",
    "* `prediction: 최종 예측 label 값`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측값 : ``accuracy=evaluator_accuracy.evaluate(predictions) ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 1.0\n"
     ]
    }
   ],
   "source": [
    "# 정확도 판단해 보기\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # 다중분류\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"target\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "# metricname 은 평가방법\n",
    "\n",
    "accuracy=evaluator_accuracy.evaluate(predictions) # 예측값이 들어있는 predictions dataframe이 들어간다.\n",
    "print(\"정확도\",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisiticRegression 사용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|       rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[18.6086266693526...|[0.99997762791224...|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[18.8180066107263...|[0.99997581287298...|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[22.6963845270307...|[0.99999942608846...|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[16.7506644665745...|[0.99971232954776...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[17.3393987944099...|[0.99969323067671...|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "정확도 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# ML 알고리즘 객체 생성\n",
    "lr =LogisticRegression(featuresCol='features',labelCol='target',maxIter=10)\n",
    "\n",
    "lr_model= lr.fit(train_feature_vector_sdf)\n",
    "\n",
    "predictions=lr_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)\n",
    "\n",
    "accuracy=evaluator_accuracy.evaluate(predictions) # 예측값이 들어있는 predictions dataframe이 들어간다.\n",
    "print(\"정확도\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라인 구축\n",
    "- 만드는 이유 : `pipeline`은 여러 개의 개별적인 `transformer`의 변환 작업, `Estimator`의 학습 작업을 일련의 process 연결을 통해 간단한 API 처리로 구현할 수 있게 만들어 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이프라인은 개별 변환 및 모델 학습 작업을 stage로 각각 정의한다.\n",
    "- 정의하고 파이프라인에 등록한다.\n",
    "- Pipeline.fit() 메소드를 활용해서 순서대로 연결된 stage 작업을 일괄적으로 수행한다.\n",
    "- Pipeline.fit()의 결과물은 pipelineModel이 반환된다.\n",
    "- PipelineModel에서 예측 작업을 transform()으로 수행한다.\n",
    "\n",
    "#### 정리\n",
    "1. stage 정의\n",
    "2. 연속적으로 이어준 stage들을 파이프라인에 등록   \n",
    "3. 파이프라인에서 fit()하면 모델이 등장\n",
    "4. 모델에서 tramsform을 통해서 prediction 작업 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# 첫번째 stage는 Feature Vectorization을 위해 VectorAssembler 객체 생성\n",
    "stage_1 = VectorAssembler(inputCols=iris_columns, outputCol='features')\n",
    "\n",
    "# 두번째 stage는 학습을 위한 모델 생성\n",
    "stage_2 = DecisionTreeClassifier(featuresCol='features', labelCol='target',maxDepth=5)\n",
    "\n",
    "# 리스트를 이용해 stage를 순서대로 묶어주기\n",
    "stages=[stage_1,stage_2]\n",
    "\n",
    "# 파이프라인에 등록\n",
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "print(type(pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.pipeline.PipelineModel'>\n"
     ]
    }
   ],
   "source": [
    "# Transformation 작업을 수행할 데이터 프레임을 fit한다.\n",
    "pipeline_model = pipeline.fit(train_sdf)\n",
    "print(type(pipeline_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
