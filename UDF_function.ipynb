{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4932b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "general-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "\n",
    "transactions = [\n",
    "    ('에그드랍', '2022-11-20 15:20:00', 15700, 'KRW'),\n",
    "    ('두마리찜닭', '2022-11-18 19:50:00', 32800, 'KRW'), \n",
    "    ('커피온리', '2022-11-19 12:25:00', 15900, 'KRW'), \n",
    "    ('뉴발란스 후리스', '2022-11-20 13:20:00', 45000, 'KRW'), \n",
    "    ('아로마티카 시카', '2022-11-20 00:01:30', 18800, 'KRW')\n",
    "]\n",
    "\n",
    "schema = [\"name\", \"datetime\", \"price\", \"currency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corrected-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data= transactions, schema=schema)\n",
    "df.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worth-beauty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----+--------+\n",
      "|           name|           datetime|price|currency|\n",
      "+---------------+-------------------+-----+--------+\n",
      "|       에그드랍|2022-11-20 15:20:00|15700|     KRW|\n",
      "|     두마리찜닭|2022-11-18 19:50:00|32800|     KRW|\n",
      "|       커피온리|2022-11-19 12:25:00|15900|     KRW|\n",
      "|뉴발란스 후리스|2022-11-20 13:20:00|45000|     KRW|\n",
      "|아로마티카 시카|2022-11-20 00:01:30|18800|     KRW|\n",
      "+---------------+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-monaco",
   "metadata": {},
   "source": [
    "# UDF\n",
    "- user defined function : 사용자 정의 함수\n",
    "- 분산 병렬 처리 환경에서 사용할 수 있는 함수를 만들어 낸다.\n",
    "    - spark에서 기본적으로 제공하지 않는 함수를 worker에서 일을 할 수 있도록 함수를 만들어준다.\n",
    "- udf를 만들지 않고 하면 마스터노드에서 일을 하는 상태이다.\n",
    "    - ``워커한테 일을 할 수 있는 함수``를 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incident-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "# 마스터 노드에서 사용하는 함수이다. worker에서 작동하지 않는다.\n",
    "def squared(n):\n",
    "    return n*n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6964e43",
   "metadata": {},
   "source": [
    "#### worker에서 함수가 작동되게 할 수 있도록 udf로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surgical-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(n)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"squared\",squared,LongType())\n",
    "# 가로 안 첫번째 문자열은 worker에서 사용할 함수의 이름 지정\n",
    "# 가로 안 두번째는 내가 작성한 함수이름을 입력\n",
    "# 가로 안 세번째 리턴타입(지정하지 않으면 디폴트 string 문자열)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accredited-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: long (nullable = true)\n",
      " |-- squared(price): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT price, squared(price) FROM transactions\").printSchema()\n",
    "# squared(price)는  spark.udf.register(\"squared\",squared,LongType())의 \"squared\"에서 온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854442c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|price|udf_amount|\n",
      "+-----+----------+\n",
      "|15700| 246490000|\n",
      "|32800|1075840000|\n",
      "|15900| 252810000|\n",
      "|45000|2025000000|\n",
      "|18800| 353440000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT price, squared(price) as udf_amount FROM transactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c59232",
   "metadata": {},
   "source": [
    "#### 숫자 한글로 변경하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blank-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_number(n):\n",
    "    units = [\"\", \"십\", \"백\", \"천\", \"만\"]\n",
    "    nums = '일이삼사오육칠팔구'\n",
    "    result = []\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        n, r = divmod(n, 10)\n",
    "        if r > 0:\n",
    "            result.append(nums[r-1]+units[i])\n",
    "        i += 1\n",
    "    return \"\".join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fantastic-manhattan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼만오천'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_number(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-detroit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.read_number(n)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"read_number\",read_number) # 리턴 타입이 생략된 것은 문자열 타입으로 리턴이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "young-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|price|read_number(price)|\n",
      "+-----+------------------+\n",
      "|15700|      일만오천칠백|\n",
      "|32800|      삼만이천팔백|\n",
      "|15900|      일만오천구백|\n",
      "|45000|          사만오천|\n",
      "|18800|      일만팔천팔백|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select price, read_number(price)\n",
    "from transactions\n",
    "'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecbfe1",
   "metadata": {},
   "source": [
    "#### 요일 구해내는 함수 만들고 등록하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confirmed-sport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_weekday(date)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요일 구해내는 함수 만들기\n",
    "def get_weekday(date):\n",
    "    import calendar\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "spark.udf.register(\"get_weekday\", get_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515a93a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monday'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "get_weekday(datetime(2022,11,21) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numeric-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|           datetime|day_of_week|\n",
      "+-------------------+-----------+\n",
      "|2022-11-20 15:20:00|     Sunday|\n",
      "|2022-11-18 19:50:00|     Friday|\n",
      "|2022-11-19 12:25:00|   Saturday|\n",
      "|2022-11-20 13:20:00|     Sunday|\n",
      "|2022-11-20 00:01:30|     Sunday|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "SELECT datetime, get_weekday(TO_DATE(datetime)) as day_of_week\n",
    "FROM transactions\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96961bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ed8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
